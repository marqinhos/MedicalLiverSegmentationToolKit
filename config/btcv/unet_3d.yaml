# DATA
data_root: /../../Datasets/BTCV2_
classes: 14
modality: CT


# MODEL
arch: unet
in_chan: 1
base_chan: 32
down_scale: [[1,2,2], [2,2,2], [2,2,2], [2,2,2]]
kernel_size: [[1,3,3], [2,3,3], [3,3,3], [3,3,3], [3,3,3]]
block: SingleConv
norm: in

# TRAIN
epochs: 800
training_size: [96, 96, 96] # training crop size. Probably should be the same as the window size. Test with [128, 128, 128] [16, 192, 192]
start_epoch: 0
num_workers: 0 ## Important # modify this if I/O or augmentation is slow
aug_device: 'cpu'

split_seed: 0 # random seed for train/test split (shuffle) before setting cross validation fold
k_fold: 5 # number of folds in cross validation

optimizer: adamw
base_lr: 0.001
betas: [0.9, 0.999]
weight_decay: 0.05  # weight decay of the optimizer
weight: [0.5, 1, 1, 1]  # weitght of each class in the loss function
rlt: 1 # relation between CE and Dice loss

scale: [0.1, 0.3, 0.3]  # scale for data augmentation  0.1 0.3 0.3
rotate: [30, 0, 0] # rotation angle for data augmentation 
translate: [0, 0, 0]
gaussian_noise_std: 0.02
additive_brightness_std: 0.7 
gamma_range: [0.5, 1.6]

print_freq: 5
iter_per_epoch: 200


# VALIDATION
ema: True
ema_alpha: 0.99
val_freq: 10 # evaluate every val_freq epochs



# INFERENCE
sliding_window: True
window_size: [96, 96, 96]


# DDP
world_size: 1
proc_idx: 0
rank: 0
port: 10000
dist_url: 'tcp://localhost:10000' # the port number here should be the same as the previous one
dist_backend: "nccl"
multiprocessing_distributed: true # if use PyTorch DDP for multi-gpu training 
reproduce_seed: 2023
